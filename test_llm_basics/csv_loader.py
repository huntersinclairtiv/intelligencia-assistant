import os
import json
import pandas as pd

from langchain.docstore.document import Document

import open_ai_util
from chroma_db_util import ChromaDB

chroma_db = ChromaDB()


def extract_columns_to_index(df):
    TEMPLATE = f"""
    {df.head().to_markdown()} \n
    The response must be a valid json list of strings. The response must not contain any explicit use of `json` keyword.
    While performing search on the above data, using semantic search makes the most sense on which columns?
    Response Syntax: ["Column_1", "Column_2"]
    """
    column_to_index = []
    try:
        column_to_index = json.loads(open_ai_util.query(TEMPLATE))
    except Exception as e:
        print('No columns are being inedexed for semantic search.')
    return column_to_index


def questions_for_indexing(df):
    TEMPLATE = f"""
    {df.head().to_markdown()} \n
    The response must be a valid json list of strings. The response must not contain any explicit use of `json` keyword.
    Provide a list of all the questions that can be answered using the given columns.
    Response Syntax:
    ["Question1", "Question2"]
    """
    ques_list = []
    try:
        ques_list = json.loads(open_ai_util.query(TEMPLATE))
        print(ques_list)
    except Exception as e:
        print('No questions are being inedexed to answer the semantic search.')
    return ques_list


def retrieve_python_code(df, quer):
    TEMPLATE = f"""
    {df.head().to_markdown()} \n

    """
    query_code = []
    try:
        query_code = open_ai_util.query(TEMPLATE)
    except Exception as e:
        print('No query generated by LLM')
    return query_code


def index_columns_for_semantic_search(df, column_to_index=[], base_metadata={}):
    document_list = []

    def process_row(row):
        for column in column_to_index:
            metadata = {
                # LOOK WHAT OTHER FIELDS WILL BE NEEDED
                'column_name': column
            }
            metadata.update(base_metadata)
            document_list.append(
                Document(page_content=row[column], metadata=metadata))

    # Apply the function to each row
    df.apply(process_row, axis=1)
    # create vectorstore from indexed data
    return document_list


def index_table_for_semantic_search(df, base_metadata, document_list=[]):
    ques_list = questions_for_indexing(df)
    for ques in ques_list:
        metadata = {
            # LOOK WHAT OTHER FIELDS WILL BE NEEDED
        }
        metadata.update(base_metadata)
        document_list.append(
            Document(page_content=ques, metadata=metadata))
    # create vectorstore from indexed data
    vectorstore = chroma_db.create_persistent_vector_database(document_list)
    return vectorstore


def process(file_path):
    df = pd.read_csv(file_path)
    columns_to_index = extract_columns_to_index(df)
    base_metadata = {
        'source': file_path,
        # JSON LOAD column_list WHEN TRYING TO RETRIEVE
        'column_list': json.dumps(columns_to_index)
    }
    # Index Columns for semantic search
    document_list = index_columns_for_semantic_search(
        df, columns_to_index, base_metadata)
    print('INDEXED COLUMNS')
    # Index Table for semantic search
    index_table_for_semantic_search(df, base_metadata, document_list)


fuzzy_function_def = '''
def fuzzy(column_name, to_match) -> str:
    """
    Returns the most relevant row from the column
    """
'''


def fuzzy(column_name, to_match) -> str:
    """
    Returns the most relevant row from the column
    """
    # vectorstore = chroma_db.get_persistent_vector_database()
    doc = ChromaDB().get_persistent_vector_database(
    ).as_retriever().get_relevant_documents(to_match)
    if doc:
        return doc[0].page_content

    print('NO RELEVANT COLUMNS FOUND FOR: ', to_match)
    exit(1)
    # FOR NOW, WE WILL JUST SEARCH FOR WITHIN A SINGLE DOC BUT LATER THIS NEEDS TO BE PASSED TO VECTORSTORE SOMEHOW


def retrieve():
    query_list = [
        "What was the gender of Miss. Bonnell"
        "Did Mrs Angle Survive? What was her age?",
        "How many males servived?",
        "List down the oldest 5 males on the ship.",
        "Can you draw any conclusions on the people who did not servive?",
        "Was the count of male deaths more than the count of all other deaths?",
        # "What was the average age of people who died?"
        # ADDITIONAL QUESTIONS CAN BE ADDED HERER
    ]
    dir_path = 'csv_outputs'
    os.makedirs(dir_path, exist_ok=True)
    vectorstore = chroma_db.get_persistent_vector_database()
    for query in query_list:
        print('Answering: ', query)
        retrieved_docs = vectorstore.as_retriever().get_relevant_documents(query)
        if retrieved_docs:
            indexed_columns = retrieved_docs[0].metadata['column_list']
            file_path = retrieved_docs[0].metadata['source']
            df = pd.read_csv(file_path)
            sample_rows = df.head().to_markdown()
            df_query = open_ai_util.generate_df_query(
                indexed_columns=indexed_columns,
                sample_rows=sample_rows,
                user_query=query,
                fuzzy_function_def=fuzzy_function_def
            )
            # df_query = df_query.strip('```python').rstrip('```').strip()
            locals = {}
            exec(df_query, globals().update({'df':df}), locals)
        else:
            print('Unable to fetch relevant information for query.')


file_path = 'titanic.csv'
process(file_path)
print('VECTOR STORE CREATED SUCCESSFULLY')
retrieve()
